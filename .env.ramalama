OPENAI_BASE_URL=http://localhost:8080/v1
OPENAI_API_KEY=unused
# TODO(ramalama): figure out how to configure for serving multiple models under
# the same endpoint.
CHAT_MODEL=qwen2.5:0.5b
# Eval model must be able larger to understand json format, and not itself
# hallucinate when scoring metrics.
EVAL_MODEL=deepseek-r1:14b

OTEL_SERVICE_NAME=testing-genai
# Change to 'false' to hide prompt and completion content
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true
# Change to affect behavior of which resources are detected. Note: these
# choices are specific to the language, in this case Python.
OTEL_EXPERIMENTAL_RESOURCE_DETECTORS=process_runtime,os,otel,telemetry_distro

# Export metrics every 3 seconds instead of every minute
OTEL_METRIC_EXPORT_INTERVAL=3000
# Export traces every 3 seconds instead of every 5 seconds
OTEL_BSP_SCHEDULE_DELAY=3000
