name: test

on:
  pull_request:
    branches:
      - main
    paths-ignore:
      - '**/*.md'
      - '**/*.png'
      - .pre-commit-config.yaml
      - lychee.toml
      - Makefile

permissions:
  contents: read

env:
  # Ensure we can see the server log if an integration test fails.
  TRAP_SERVER_LOG: "trap 'if [ $? -ne 0 ]; then cat server.log; fi' ERR"
  TRAP_PARENT_SERVER_LOG: "trap 'if [ $? -ne 0 ]; then cat ../server.log; fi' ERR"

jobs:
  test:
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: 3.13

      - name: 06-http-replay
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pytest -m vcr --vcr-record=none
        working-directory: 06-http-replay

  # This runs integration tests, using a local inference server instead of the
  # OpenAI platform. This lets us test PRs from forks which can't access secrets
  # like API keys.
  integration-test:
    name: "integration-test (${{ matrix.name }})"
    runs-on: ubuntu-24.04
    strategy:
      matrix:
        include:
          - name: "ollama"
            pull: "after"  # ollama pulls via the server
            serve: "ollama serve"
            health: http://127.0.0.1:11434
          - name: "ramalama"
            pull: "before"  # ramalama pulls directly into cache
            serve: "dotenv run -- sh -c 'ramalama --nocontainer serve ${CHAT_MODEL}'"
            health: http://127.0.0.1:8080/health
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: 3.13
      - name: Install dotenv and openai
        run: |
          python -m pip install --upgrade pip
          pip install openai 'python-dotenv[cli]'
      - name: Install Ollama
        if: ${{ matrix.name == 'ollama' }}
        run: curl -fsSL https://ollama.com/install.sh | sh
      - name: Install RamaLama
        if: ${{ matrix.name == 'ramalama' }}
        # In nocontainer mode, RamaLama runs llama-server processes directly.
        # We install it from the latest llama.cpp release.
        run: |
          pip install ramalama
          TAG=$(gh release view --repo ggml-org/llama.cpp --json tagName --jq .tagName)
          gh release download $TAG --repo ggml-org/llama.cpp --pattern "llama-${TAG}-bin-ubuntu-x64.zip" --dir .
          unzip "llama-${TAG}-bin-ubuntu-x64.zip" -d ${{ github.workspace }}/llama.cpp
          echo "${{ github.workspace }}/llama.cpp/build/bin" >> $GITHUB_PATH
          echo "LD_LIBRARY_PATH=${{ github.workspace }}/llama.cpp/build/bin" >> $GITHUB_ENV
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Create .env file
        run: |
          (cat .env.${{ matrix.name }}; echo; cat .env.otel.console) > .env
      # Depending on the server, we pull the model before or after it starts.
      # Regardless, we block on health before proceeding to the next step.
      - name: Start server
        run: |
          PULL_MODEL="dotenv run -- sh -c '${{ matrix.name }} pull \${CHAT_MODEL}'"
          if [ "${{ matrix.pull }}" = "before" ]; then eval ${PULL_MODEL}; fi
          nohup ${{ matrix.serve }} > server.log 2>&1 &
          ${{ env.TRAP_SERVER_LOG }}
          time curl --retry 10 --retry-connrefused --retry-delay 2 -sf ${{ matrix.health }}
          if [ "${{ matrix.pull }}" = "after" ]; then eval ${PULL_MODEL}; fi
      - name: Smoke test the model using OpenAI
        run: |  # Use the same command as 01-start, to test the OpenAI base URL.
          ${{ env.TRAP_SERVER_LOG }}
          dotenv run -- sh -c 'openai api chat.completions.create \
            -t 0 -m ${CHAT_MODEL} \
            --message user "Answer in up to 3 words: Which ocean contains Bouvet Island?"'
      - name: 05-test
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          ${{ env.TRAP_PARENT_SERVER_LOG }}
          pytest
        working-directory: 05-test
      - name: 06-http-replay
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          ${{ env.TRAP_PARENT_SERVER_LOG }}
          pytest -m integration
        working-directory: 06-http-replay
      - name: 07-eval
        run: |  # not "-m eval" as the EVAL_MODEL is too big for CI
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          ${{ env.TRAP_PARENT_SERVER_LOG }}
          pytest -m integration
        working-directory: 07-eval
